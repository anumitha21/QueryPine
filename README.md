

# ğŸ“˜ QueryPine

> âš¡ï¸ A powerful **document-based question answering system** built with **Pinecone vector database**, **Google Generative AI embeddings**, and **Groq's Llama model**.
> Upload PDF documents and ask questions about their content using **advanced AI capabilities**.

---

## âœ¨ Features

 **PDF Document Processing** â€“ Load and process multiple PDF documents from a directory
 **Text Chunking** â€“ Intelligent text splitting with configurable chunk sizes and overlaps
 **Vector Embeddings** â€“ Google Generative AI embedding model for high-quality vector representations
 **Vector Database** â€“ Pinecone integration for efficient vector storage & similarity search
 **AI-Powered Q\&A** â€“ Groqâ€™s Llama 3.3 70B model for accurate question answering
 **Retry Mechanism** â€“ Built-in retry logic for reliable Pinecone operations

---

## ğŸ› ï¸ Prerequisites

*  Python **3.8+**
*  Pinecone account + API key
*  Google Generative AI API key
*  Groq API key

---

## âš¡ Installation

```bash
# Clone the repository
git clone <repository-url>
cd chatbot-pinecone

# Install dependencies
uv add -r requirements.txt
```

---

## âš™ï¸ Setup

1. Create a **`.env` file** in the root directory with your API keys:

```env
GOOGLE_API_KEY=your_google_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
GROQ_API_KEY=your_groq_api_key_here
```

2. Place your **PDF documents** inside the `document/` directory
3. Update Pinecone configuration in `test.ipynb` if needed:

   * `index_name` â†’ Name of your Pinecone index
   * `environment` â†’ Your Pinecone environment
   * `region` â†’ Your Pinecone region

---

## â–¶ï¸ Usage

### ğŸ““ Running the Notebook

1. Open **`test.ipynb`** in Jupyter Notebook/Lab
2. Run cells sequentially to:

   * Load and process PDF documents
   * Create embeddings and store in Pinecone
   * Query the system with questions

#### ğŸ” Example

```python
# Load documents
docs = read_doc('document/')

# Process and chunk documents
document = chuck_data(docs=docs)

# Create embeddings and store in Pinecone
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
index = PineconeVectorStore.from_documents(
    documents=docs,
    embedding=embeddings,
    index_name="my-embedding-index"
)

# Ask questions
query = "What is the shift of llms after the transformers rise ?"
answer = retrieve_answers(query)
print(answer)
```

---

### ğŸ’» Running the Main Application

```bash
python main.py
```

---

## ğŸ“‚ Project Structure

```
chatbot-pinecone/
â”œâ”€â”€ document/              # PDF documents
â”œâ”€â”€ test.ipynb             # Notebook with implementation
â”œâ”€â”€ main.py                # Main application
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ pyproject.toml         # Project config
â”œâ”€â”€ uv.lock                # Dependency lock
â”œâ”€â”€ .python-version        # Python version
â”œâ”€â”€ .gitignore             # Ignore rules
â””â”€â”€ README.md              # This file
```

---

## ğŸ“¦ Dependencies

*  **langchain** â€“ LLM framework
*  **pinecone** â€“ Vector database
*  **langchain-groq** â€“ Groqâ€™s LLM integration
*  **langchain-google-genai** â€“ Google GenAI integration
*  **pypdf** â€“ PDF document processing
*  **python-dotenv** â€“ Manage environment variables

---

## ğŸ”‘ API Keys Required

1. **Google Generative AI** â†’ For text embeddings
2. **Pinecone** â†’ Vector database storage
3. **Groq** â†’ LLM inference with Llama model

---

## âš™ï¸ Configuration

ğŸ“Œ **Pinecone Settings**

* Index Name: `my-embedding-index`
* Dimension: **768** (Googleâ€™s embedding-001 model)
* Metric: **Cosine similarity**
* Environment: `us-east-1` (adjust as needed)

ğŸ“Œ **Model Settings**

* Embedding Model: `models/embedding-001` (Google)
* LLM Model: `llama-3.3-70b-versatile` (Groq)
* Temperature: **0.5**

---

## ğŸ Troubleshooting

 **API Key Issues** â†’ Ensure `.env` file has correct keys
 **Pinecone Index Errors** â†’ Verify index name & environment
 **Document Not Loading** â†’ Place PDFs in `document/` folder
 **Dependency Errors** â†’ Run `pip install -r requirements.txt`

---

## ğŸ“œ License

Licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributions

Contributions are welcome ğŸ‰!
Feel free to fork, open issues, and submit PRs ğŸš€

---

ğŸ‘‰ Would you like me to also add **shields.io badges** (for Python version, license, dependencies) and a **demo screenshot/GIF section** to make it look even more professional?
=======


# ğŸ“˜ QueryPine

> âš¡ï¸ A powerful **document-based question answering system** built with **Pinecone vector database**, **Google Generative AI embeddings**, and **Groq's Llama model**.
> Upload PDF documents and ask questions about their content using **advanced AI capabilities**.

---

## âœ¨ Features

 **PDF Document Processing** â€“ Load and process multiple PDF documents from a directory
 **Text Chunking** â€“ Intelligent text splitting with configurable chunk sizes and overlaps
 **Vector Embeddings** â€“ Google Generative AI embedding model for high-quality vector representations
 **Vector Database** â€“ Pinecone integration for efficient vector storage & similarity search
 **AI-Powered Q\&A** â€“ Groqâ€™s Llama 3.3 70B model for accurate question answering
 **Retry Mechanism** â€“ Built-in retry logic for reliable Pinecone operations

---

## ğŸ› ï¸ Prerequisites

*  Python **3.8+**
*  Pinecone account + API key
*  Google Generative AI API key
*  Groq API key

---

## âš¡ Installation

```bash
# Clone the repository
git clone <repository-url>
cd chatbot-pinecone

# Install dependencies
uv add -r requirements.txt
```

---

## âš™ï¸ Setup

1. Create a **`.env` file** in the root directory with your API keys:

```env
GOOGLE_API_KEY=your_google_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
GROQ_API_KEY=your_groq_api_key_here
```

2. Place your **PDF documents** inside the `document/` directory
3. Update Pinecone configuration in `test.ipynb` if needed:

   * `index_name` â†’ Name of your Pinecone index
   * `environment` â†’ Your Pinecone environment
   * `region` â†’ Your Pinecone region

---

## â–¶ï¸ Usage

### ğŸ““ Running the Notebook

1. Open **`test.ipynb`** in Jupyter Notebook/Lab
2. Run cells sequentially to:

   * Load and process PDF documents
   * Create embeddings and store in Pinecone
   * Query the system with questions

#### ğŸ” Example

```python
# Load documents
docs = read_doc('document/')

# Process and chunk documents
document = chuck_data(docs=docs)

# Create embeddings and store in Pinecone
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
index = PineconeVectorStore.from_documents(
    documents=docs,
    embedding=embeddings,
    index_name="my-embedding-index"
)

# Ask questions
query = "What is the shift of llms after the transformers rise ?"
answer = retrieve_answers(query)
print(answer)
```

---

### ğŸ’» Running the Main Application

```bash
python main.py
```

---

## ğŸ“‚ Project Structure

```
chatbot-pinecone/
â”œâ”€â”€ document/              # PDF documents
â”œâ”€â”€ test.ipynb             # Notebook with implementation
â”œâ”€â”€ main.py                # Main application
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ pyproject.toml         # Project config
â”œâ”€â”€ uv.lock                # Dependency lock
â”œâ”€â”€ .python-version        # Python version
â”œâ”€â”€ .gitignore             # Ignore rules
â””â”€â”€ README.md              # This file
```

---

## ğŸ“¦ Dependencies

*  **langchain** â€“ LLM framework
*  **pinecone** â€“ Vector database
*  **langchain-groq** â€“ Groqâ€™s LLM integration
*  **langchain-google-genai** â€“ Google GenAI integration
*  **pypdf** â€“ PDF document processing
*  **python-dotenv** â€“ Manage environment variables

---

## ğŸ”‘ API Keys Required

1. **Google Generative AI** â†’ For text embeddings
2. **Pinecone** â†’ Vector database storage
3. **Groq** â†’ LLM inference with Llama model

---

## âš™ï¸ Configuration

ğŸ“Œ **Pinecone Settings**

* Index Name: `my-embedding-index`
* Dimension: **768** (Googleâ€™s embedding-001 model)
* Metric: **Cosine similarity**
* Environment: `us-east-1` (adjust as needed)

ğŸ“Œ **Model Settings**

* Embedding Model: `models/embedding-001` (Google)
* LLM Model: `llama-3.3-70b-versatile` (Groq)
* Temperature: **0.5**

---

## ğŸ Troubleshooting

 **API Key Issues** â†’ Ensure `.env` file has correct keys
 **Pinecone Index Errors** â†’ Verify index name & environment
 **Document Not Loading** â†’ Place PDFs in `document/` folder
 **Dependency Errors** â†’ Run `pip install -r requirements.txt`

---

## ğŸ“œ License

Licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributions

Contributions are welcome ğŸ‰!
Feel free to fork, open issues, and submit PRs ğŸš€

---

ğŸ‘‰ Would you like me to also add **shields.io badges** (for Python version, license, dependencies) and a **demo screenshot/GIF section** to make it look even more professional?

